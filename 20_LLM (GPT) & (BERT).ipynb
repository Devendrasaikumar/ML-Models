{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f29515d-c3bb-4762-bd17-90bec15a5e54",
   "metadata": {},
   "source": [
    "# <font color='Blue'>LLM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e3089-dc94-4947-a58c-228923be7055",
   "metadata": {},
   "source": [
    "- **L**arge **L**anguage **M**odels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79057f19-be2c-4593-bd26-7574bdb10c7f",
   "metadata": {},
   "source": [
    "-  A Large Language Model (LLM) is an artificial intelligence model trained on vast amounts of text data to understand, generate, and manipulate human language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e76d8-5757-46c8-9b0a-45eefdbda901",
   "metadata": {},
   "source": [
    "**Key Characteristics of LLMs**:\n",
    "\n",
    "- *Scale*: LLMs are \"large\" because they have billions (or even trillions) of parameters that help them learn complex language patterns.\n",
    "  \n",
    "- *Pre-trained*: These models are pre-trained on large datasets, which means they can perform a variety of language tasks without needing task-specific training.\n",
    "  \n",
    "- *Generative and Predictive*: LLMs can generate new text or predict what comes next in a sentence, making them useful in content creation, answering questions, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbae0b-0994-4b44-bc2f-222be4315dc9",
   "metadata": {},
   "source": [
    "**LLM can do tasks like**\n",
    "\n",
    "- Answer Questions\n",
    "- Text generation\n",
    "- Text Summarization\n",
    "- Text Completion\n",
    "- Text Correction\n",
    "- Sentiment Analysis\n",
    "- Code Writting\n",
    "- Conversation\n",
    "- Creative Writting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1d097-5ce1-453b-a714-5cadb907e00e",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae36911-f76e-4df5-a2aa-a4a1cf1ba36e",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fa8e3-3c3f-4ea7-bb41-a4a16700f290",
   "metadata": {},
   "source": [
    "##### <font color='magenta'>LLM Models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84fb11-4718-46a3-a6f2-b9bf8b1ed942",
   "metadata": {},
   "source": [
    "- **GPT (Generative Pre - Trained Tranformer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27d0ec-999d-4a94-9fa4-f9804bf2b73c",
   "metadata": {},
   "source": [
    "- **BERT (Bidirectional Encoder Represantions from Tranformer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b0179-999d-4cf7-a133-c0d2d6b6e6be",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d5eec-a678-4d05-aedf-ddc69bf54627",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53fe2a3-b113-4793-a94e-dbc3b56ac49f",
   "metadata": {},
   "source": [
    "### <font color='green'>GPT (Generative Pre - Trained Tranformer)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa5fd8-cf61-4fb1-a27d-b1b907b83cab",
   "metadata": {},
   "source": [
    "- **G**enerative **P**retrained **T**ransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8ad83-6069-4384-85d3-35c1b0119864",
   "metadata": {},
   "source": [
    "- It is a type of LLM developed by **Open AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3a0ad-95f6-4980-9887-f77ec1d7d1ac",
   "metadata": {},
   "source": [
    "- It is designed to respond to humans based on the input they provide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1254f7c-2ec2-442c-abeb-6d41dbf3b575",
   "metadata": {},
   "source": [
    "##### <font color='magenta'>How GPT Model Works:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2568d-9a69-46db-a8ad-dd4d3b36013c",
   "metadata": {},
   "source": [
    "**1.Training**:\n",
    "\n",
    "**Pretraining**: The GPT model is trained on massive amounts of text data (from books, websites, articles, etc.). During this phase, the model learns patterns in language—such as grammar, sentence structure, word meanings, and contextual relationships between words.\n",
    "\n",
    "**Objective**: The model learns to predict the next word in a sequence of words, given the previous context. \n",
    "\n",
    "  For example, if given the phrase “The sky is _____,” it might predict \"blue\" as the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81bcf8-8add-4d6f-9f1d-4772bbe29a8b",
   "metadata": {},
   "source": [
    "**2.Transformer Architecture**:\n",
    "\n",
    "- The core of GPT is the Transformer architecture, which uses self-attention mechanisms to understand relationships between words in a sentence, regardless of how far apart they are. This enables GPT to generate coherent text over longer passages.\n",
    "\n",
    "- **Self-Attention**: This mechanism allows the model to weigh the importance of each word in the context of the entire sequence, rather than processing them in a strictly sequential manner (like older models such as RNNs or LSTMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b615c-e031-4003-b260-65c23fa7b2e1",
   "metadata": {},
   "source": [
    "**3.Fine-tuning (Optional)**:\n",
    "\n",
    "After pretraining, GPT can be fine-tuned for specific tasks (e.g., summarization, translation, or customer service) using a smaller, task-specific dataset. Fine-tuning allows GPT to specialize in various tasks while still leveraging its pre-trained knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3232325d-3bba-4e48-95f4-6ae4dd713292",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3387cc8-d458-42dc-85c9-e63a399d6bef",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ec9d9-b200-48ee-ba68-8999254269ed",
   "metadata": {},
   "source": [
    "##### <font color='magenta'>How GPT Runs in the Backend: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8c700-ab0a-4872-8edb-65023f6ab381",
   "metadata": {},
   "source": [
    "1 . **Input (Prompt)**:\n",
    "\n",
    "The process begins when a user provides a prompt or input (e.g., \"Tell me a joke\" or \"What is the weather today?\"). This input is sent to the backend where the GPT model resides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29623f2b-5007-4c5b-a194-52b5feb917a9",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af28e3-1c2e-4a15-89c8-9f9ba3204c07",
   "metadata": {},
   "source": [
    "2. **Tokenization**:\n",
    "\n",
    "The input is first broken down into tokens, which are small units of meaning (e.g., words or sub-words). For instance, the sentence \"I love cats\" might be split into tokens like [\"I\", \"love\", \"cats\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba634eb-51d6-47cb-9b6b-1b4de3756e4f",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01610a04-1fb2-420f-9c89-967c445ce9b6",
   "metadata": {},
   "source": [
    "3. **Encoding**:\n",
    "\n",
    "The tokens are passed through the model's layers (a series of mathematical operations) in which each token is processed with respect to the others. GPT uses multiple layers of self-attention to learn contextual relationships between the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c48128-05be-4788-810f-f74205d83a58",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09260bba-cfc8-4709-b930-8b751a5d1e03",
   "metadata": {},
   "source": [
    "4. **Generation**:\n",
    "\n",
    "After encoding the input, GPT generates the next set of tokens (words or sub-words). The model uses its learned parameters (from training) to predict the most likely continuation of the sequence based on the context.\n",
    "The generated tokens are decoded back into human-readable text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2b47b-683f-477e-9cc8-d3037972f070",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aaa539-8f43-4903-b138-551c19de0fc9",
   "metadata": {},
   "source": [
    "5. **Output**:\n",
    "\n",
    "The generated text is returned to the user as the model's response to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb171f5-b8c9-4997-b49b-9dce39da0e35",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c2b33-4d81-465d-bcba-06d0039d1983",
   "metadata": {},
   "source": [
    " ##### <font color='magenta'>**Example of GPT Model in Action**: </font>\n",
    "\n",
    "**User Input**: \"What is the capital of France?\"\n",
    "\n",
    "**Backend Processing**:\n",
    "\n",
    "The input is tokenized, e.g., [\"What\", \"is\", \"the\", \"capital\", \"of\", \"France\", \"?\"].\n",
    "\n",
    "The model processes the tokens through its transformer layers, considering the relationships between each token.\n",
    "\n",
    "The model predicts that the next word after the tokens \"capital of France\" is most likely \"Paris,\" given that it has learned from \n",
    "vast data that \"Paris\" is commonly associated with the capital of France.\n",
    "\n",
    "**Output**:\n",
    "\n",
    "The model returns the response: \"The capital of France is Paris.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25397be-e26a-4c0e-8fc2-528f4fdffcaf",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1844b5e-0a83-4b2b-9a3d-47fa6239c3f6",
   "metadata": {},
   "source": [
    " ##### <font color='magenta'>**Example of GPT for Text Generation**:  </font>\n",
    "\n",
    "**User Input**: \"Write a short story about a dragon who loves to sing.\"\n",
    "\n",
    "**Backend Processing**:\n",
    "\n",
    "The input is tokenized and processed through the transformer layers.\n",
    "\n",
    "The model generates tokens based on the context of the prompt and previous text, using its learned knowledge to produce a coherent narrative.\n",
    "\n",
    "The model might generate a response like: \"Once upon a time, there was a dragon named Draco who loved to sing. Every evening, he would climb to the top of a mountain and belt out beautiful melodies that echoed across the land...\"\n",
    "\n",
    "**Output**:\n",
    "\n",
    "The full generated short story is returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec262a4-2fd8-4a43-8239-ef474e4312fe",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fed03-eb54-4e0c-ace1-140358c34819",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148217f-761d-42a2-8c45-87ef6cb210eb",
   "metadata": {},
   "source": [
    "##### <font color='magenta'>Evolution of GPT models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816fbf3-0af4-403c-ac19-9dff2a14cb00",
   "metadata": {},
   "source": [
    "- **GPT -1** (2018) :- Learned 117 million parameters\n",
    "- **GPT -2** (2019) :- Learned 1.5 billion parameters\n",
    "- **GPT -3** (2020) :- Learned 175 billion parameters\n",
    "- **GPT -4** (2023) :- More accurate response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a671237-69e0-4338-af3c-0b84d09e2be0",
   "metadata": {},
   "source": [
    "### <font color='green'></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fc2ab-c9c0-4f43-b0e1-043c7c65fba3",
   "metadata": {},
   "source": [
    "### <font color='green'></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcbb32-79e5-4758-ad9f-bfb41f745037",
   "metadata": {},
   "source": [
    "### <font color='green'>BERT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ffc84-0b5f-4559-8228-888ac0031470",
   "metadata": {},
   "source": [
    "- **B**idirectional **E**ncoder **R**epresentations from **T**ransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a3e0d-ee03-4fac-95a2-b29562a7c2f2",
   "metadata": {},
   "source": [
    "- Develpoed by google in 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90246962-4d5d-4b96-929f-0ae2ba5a2807",
   "metadata": {},
   "source": [
    "- BERT is Bidirectional, it reads the entire sentance at once, considering both left and right sides oa a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203343f2-9d58-47cf-87e3-9aa24adc873b",
   "metadata": {},
   "source": [
    "- In this transfomer architeture also use to know the importance of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a816095c-d972-41e1-a1df-fd1481bd660d",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b5a1f-ed24-4b71-abe9-f6a081fb5b1e",
   "metadata": {},
   "source": [
    " ##### <font color='magenta'>**Key Features of BERT** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c58b1-5a8b-4081-ae40-8afb8b3f2505",
   "metadata": {},
   "source": [
    "1.**Bidirectional Contextual Understanding**:\n",
    "\n",
    "- Traditional models like GPT process text in a unidirectional manner, meaning they predict the next word based on previous words (left to right or right to left).\n",
    "- In contrast, BERT processes text in a bidirectional way, meaning it looks at the entire sequence of words (both to the left and right of a word) to understand its meaning. This gives it a richer and more nuanced understanding of context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1af295-5c30-4039-acc4-b576a835b04b",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7cba6-e55c-41c2-a55b-c1217868621f",
   "metadata": {},
   "source": [
    "**Masked Language Model (MLM)**:\n",
    "\n",
    "- Pretraining Task: BERT is trained using a method called Masked Language Modeling (MLM). During training, some words in a sentence are randomly masked (replaced by a special token, like [MASK]), and the model’s task is to predict these masked words based on the surrounding context.\n",
    "- Example: In the sentence \"The cat sat on the [MASK].\" BERT learns to predict that the missing word is likely \"mat,\" based on the surrounding words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174ca14-c74a-4cc8-aa3b-8fff79be1721",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a517e-d7e2-45ae-b8d2-0250f4851142",
   "metadata": {},
   "source": [
    "**Next Sentence Prediction (NSP)**:\n",
    "\n",
    "- Another unique pretraining task for BERT is Next Sentence Prediction. During training, the model is given pairs of sentences and has to predict whether the second sentence is the actual next sentence in the text or a random sentence.\n",
    "- Example: If Sentence A is \"I went to the store\" and Sentence B is \"I bought some milk,\" BERT learns to predict that Sentence B is likely the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff51cd-832f-495c-8524-05a0eddc7df4",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88db97-edf2-4ae5-a946-da72bbe4d48a",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a03d3e-f17b-4f2e-b667-76cf94f0f369",
   "metadata": {},
   "source": [
    "##### <font color='magenta'>How BERT Works: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b601e3-024c-43c2-9cd2-c9ba8fb84d07",
   "metadata": {},
   "source": [
    "1. **Training**:\n",
    "\n",
    "- BERT is pretrained on a large corpus of text, such as Wikipedia and BookCorpus.\n",
    "In Masked Language Modeling (MLM), a portion of words in a sentence is replaced with a [MASK] token, and the model is tasked with predicting the original word.\n",
    "- In Next Sentence Prediction (NSP), the model is trained to determine whether a second sentence logically follows the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907cb68c-1de3-4610-995c-261d4169d2cf",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778aea6-2c8b-49e4-a3f5-a94a74304f98",
   "metadata": {},
   "source": [
    "2. **Fine-tuning**:\n",
    "\n",
    "- Once pretraining is complete, BERT can be fine-tuned for specific downstream tasks (e.g., sentiment analysis, question answering, named entity recognition).\n",
    "- Fine-tuning involves training BERT on a smaller, task-specific dataset while leveraging the knowledge from the pretraining phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e809db43-3c98-41c4-bc4c-a217753a090e",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de531d2-19ca-4261-b35e-df002c012488",
   "metadata": {},
   "source": [
    "3. **Bidirectional Encoding**:\n",
    "\n",
    "- BERT uses the Encoder part of the Transformer architecture, which is focused on understanding the input sequence. The encoder is bidirectional, meaning it looks at the full context (both left and right of each word) when understanding a word’s meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dbd451-eab5-42a1-9252-d74cd28cd2c4",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1da40-9be4-4dbf-9865-b0103625bae9",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153e997-d13c-490e-8fba-dbc3b3bdfc7a",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> Uses of this BERT </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810f053-0112-4524-b4da-b58e93804b7d",
   "metadata": {},
   "source": [
    "- Text Classification\n",
    "- Question Answering\n",
    "- Named Entity Recognition\n",
    "- Sentiment Analysis\n",
    "- Language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8875e3d-31f2-41f6-86bc-6a4c480823d5",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf64927-5476-4763-a308-2a7865df3a24",
   "metadata": {},
   "source": [
    "# <font color='red'>Model Building for two Models  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55eed2f1-825f-474d-bff3-a8c146652350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Input text prompt\n",
    "input_text = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using GPT-2\n",
    "outputs = model.generate(inputs, max_length=100)\n",
    "\n",
    "# Decode the generated tokens to get the output text\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0cfd6e9-b378-4209-887d-8240844a7fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model for classification\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "input_text = \"I love machine learning!\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Make prediction (classification)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the predicted class\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4c7d7-26a1-4eba-9a1c-c60dd238276a",
   "metadata": {},
   "source": [
    "##### <font color='magenta'> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2d7a5-7ca1-45e1-a872-2c42cc09fd82",
   "metadata": {},
   "source": [
    "# <font color='red'>END.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
